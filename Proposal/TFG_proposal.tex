\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{
    a4paper,
    left = 18mm,
    right = 18mm,
    top = 14mm,
}

\usepackage{filecontents}
\begin{filecontents}{refs.bib}
@article{bib:REACT,
    title = "REACT: reactive resilience for critical infrastructures using graph-coloring techniques",
    journal = "Journal of Network and Computer Applications",
    volume = "145",
    pages = "102402",
    year = "2019",
    issn = "1084-8045",
    doi = "https://doi.org/10.1016/j.jnca.2019.07.003",
    url = "http://www.sciencedirect.com/science/article/pii/S1084804519302279",
    author = "Ivan Marsa-Maestre and Jose Manuel Gimenez-Guzman and David Orden and Enrique {de la Hoz} and Mark Klein",
    keywords = "Network security, Network theory (graphs), Optimization, Simulated annealing",
    abstract = "Nowadays society is more and more dependent on critical infrastructures. Critical network infrastructures (CNI) are communication networks whose disruption can create a severe impact. In this paper we propose REACT, a distributed framework for reactive network resilience, which allows networks to reconfigure themselves in the event of a security incidents so that the risk of further damage is mitigated. Our framework takes advantage of a risk model based on multilayer networks, as well as a graph-coloring problem conversion, to identify new, more resilient configurations for networks in the event of an attack. We propose two different solution approaches, and evaluate them from two different perspectives, with a number of centralized optimization techniques. Experiments show that our approaches outperform the reference approaches in terms of risk mitigation and performance."
}
@article{bib:ICS,
    title={Improving ICS Cyber Resilience through Optimal Diversification of Network Resources}, 
    author={Tingting Li and Cheng Feng and Chris Hankin},
    year={2019},
    eprint={1811.00142},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}
\end{filecontents}

\usepackage{cite}

\title{\vspace{-1cm}TFG Proposal}
\author{Pablo Collado Soto \\ \\ \textbf{Universidad de Alcalá}}
\date{}

\begin{document}
    \maketitle

    \section{Introduction}
        The object of the proposed End of Degree Project (\textit{EDP}, \textit{TFG} in Spanish) sits within the scope of the \textit{DNA} project (\texttt{TIN2016-80622-P}) from the \textit{Automatics Department in the University of Alcalá}, whose main focus is the study of negotiation mechanisms for the optimizing of behaviours in complex selfish networks.\\

        Several suitable scenarios for the application of the techniques derived derived from the above have been identified. One of the most promising is the \textit{reactive resilience} of critical networks. This would confer a network the ability of reconfiguring itself in the face of an attack so that it poses a higher degree of resistance to the attack itself. The reconfiguration itself implies a tolerable degradation of the service offered by the network which we can safely assume provided the network's resistance is indeed enhanced. The experiments carried out on a mathematical models of the networks have shown a better risk reduction and performance than the reference implementations as seen in \cite{bib:REACT}.\\

        The previous discussion deals with more abstract and conceptual models. In an effort to increase the applicability of the work put forward in \cite{bib:REACT}, the development of a virtualized network infrastructure that simulates that of a hospital was proposed. This testbed could later be used to study the effect of the techniques proposed in \cite{bib:REACT} so that we can analyze their performance.\\

    \section{Objectives and field of application}
        We have already described the project our \textit{EDP} belongs to. Now, the objective we pursue is none other than the development of a virtualized network infrastructure that can be employed as a testbed supporting the \textit{REACT} project.\\

        Throughout the development of said infrastructure we have tried to make our design as modular as possible. Even though the main use of our platform will be testing network resilience strategies we believe its applications might be further reaching. At the end of the day, our work might be considered like a ``black box'' that takes a graph describing a network as an input and then generates a virtualized network that mimics that description. This can then be employed for purposes as diverse as testing network protocol performance on networks with an arbitrary topology to being used as a learning environment for network architecture students who want to study how routing protocols work or how to configure firewall policies to attain a certain service policy.\\

        Given the flexibility of the tool we have developed, its virtual use cases are as numerous as we can seemingly think of.\\

    \section{Project description}
        At its heart, a network is nothing more than a collection of hosts or end-systems interconnected by several packet forwarders. These packet forwarders are usually referred to as \textit{routers} when they deal with \textit{IP datagrams} at the \textit{network} layer, or as \textit{switches} when they deal with \textit{frames} at the \textit{link} layer. Then, our task is to virtualize these three types of nodes and then interconnect them according to the topology described by a user through an input graph.\\

        In the age of networked services we have seen technologies such as \textit{containers} rise. One of the best known implementations of this container technology is \textit{Docker}. We therefore chose to model our hosts and routers as simple docker containers running an image of the ever pervasive \textit{Ubuntu} operating system. The choice of containers over full-fledged virtual machines is not an arbitrary one for virtualizing the end systems of a network. Whilst a virtual machine chooses to run a whole \textit{kernel} on top of the one supporting the host machine, containers leverage the preexisting kernel and only isolate the process we run within them. We can then consider virtual machines to virtualize a host at the operating system level whilst a container works at the process level. The difference in approaches makes containers a fairly low weight option when compared with virtual machines. Hence, using containers as hosts and routers makes our virtual networks that much more scalable as we won't be needing as many resources when the number of nodes begins to increase. What's more, a container runs a \textit{container image} through which the user can define what he or she want the container to run. This makes our approach tremendously flexible as the hosts and routers need not only run traditional operating systems: they can perform however the end user likes.\\

        Our project leverages the flexibility of the \textit{Linux} network stack for getting all the network infrastructure up and running. The \texttt{iproute2} suite of tools offer a \textit{command line interface} to interact with the way packets are processed within a Linux system so that we can tweak it to our needs. We can take advantage of the flexibility offered by this collection of tools to generate our own virtual network infrastructure for interconnecting the different nodes we have modeled as containers. We are then employing \texttt{iproute2}'s power to instantiate virtual point-to-point links known as \textit{veths} as well as link layer switches which are called \textit{bridges} in the Linux realm. Then, by calling \texttt{iproute2} where we need to we can instantiate all the virtual network infrastructure we might need.\\

        We now know how we'll instantiate the different nodes we might need, but we still need to take care of two aspects. First of all, we need to somehow orchestrate the instantiation process so that we don't end up trying to connect two nodes that don't yet ``exist'' for instance. We will also have to \textit{parse} the network description provided as a graph to make sense out of it and then proceed to network instantiation. We shan't forget that the aim of this infrastructure is providing a testbed for analyzing the performance of the techniques put forward in \cite{bib:REACT}. This implies that the network will need to reconfigure itself which ultimately implies we will need to move nodes around. These dynamic changes will entail changes in the network configuration: forwarding tables in routers will need tweaking and \texttt{IP} addressing will need to be revisited, for instance. Being able to respond to these requirements means we need to keep track of some sort of network state. This aspect is something we need to take into account when designing our software architecture: we need to dynamically change the network, it's not only a matter of just bringing everything up.\\

        In an effort to simplify the network's operation so that the overhead introduced by the router's was kept to a minimum we decided not to run any routing algorithms on them such as \texttt{OSPF}. Thus, we have had to manually route the network so that the correct routes providing the desired connectivity could be put in place. On top of not having any automatic routing we also chose not to run any \texttt{DHCP} server's or clients. This forced us to address each network manually with the associated overhead of keeping tack of each subnet so that we could react to changes. We also implemented a firewall on the routers through \textit{iptables}. This made us have to deal with instantiating firewall rules according to the connectivity defined by the user. We can then conclude that we are not only concerned with ``bringing the network up'', we also need to face a great deal of subtleties derived from the desired simplicity of our nodes as well as from the great deal of control over the network's functioning we are offering the user.\\

        All in all, we have tried to generate a framework for virtualizing arbitrarily complex network topologies. In order to meet the requirements imposed by the virtualized infrastructure's main objective we have had to add a \textit{control layer} to keep track of the network's state so that we could dynamically alter its topology. Having extensibility and flexibility in mind at all times, we are giving the user the freedom to choose whatever container image they want to run as hosts so that the topology can be employed for whatever means he or she deems necessary. We then conclude we have generated a system that, even though it's designed for a particular purpose, can accommodate very different use cases given it's great flexibility.\\

    \section{Development plan}
        The task at hand is not a small one. In order to maximize our chances of success we have broken down the development process into smaller pieces we can safely tackle:

        \begin{enumerate}
            \item \textbf{Getting to know the technologies}: As seen on the project description, we need to orchestrate several technologies together to get a working virtual network. What's more, we also need to study preexisting solutions for coordinating containers such as \textit{Kubernetes} as they might offer good options. In any case, we need to bend the technologies we are to use to their very limits to pull off what we are trying to achieve. This implies we'll need to have a very clear picture of the capabilities of each of them and how we can coordinate them to make their interactions as seamless as possible. This will require us to get acquainted with each of them through their documentation as well as trough small scale tests to assess whether we can employ them in the final solution.
            \item \textbf{Designing the system}: Once we have a clear idea of the technologies to use we need to develop a software solution that will automatically take care of virtualizing a network. In a first approach we won't be using graphs to describe the network so as to simplify the first iteration of our solution. We'll then try to generate a very simple network to assess whether we are on the right track. This step will lay the foundations for a management system that will be able to cope with ever more complex topologies. We expect to have to revisit the previous step during the development of this first simple topology as we are certain we'll stumble upon obstacles we hadn't foreseen before starting the design.
            \item \textbf{Enhancing the design}: We are certain that the addition of new functionalities on top of those defined for the first approach will raise unforeseen problems. Solving these will require rethinking parts of the initial design so that we can accommodate the code structure to the needs of the virtual network. We'll tackle more complex topologies in this section as well as the ability of defining network topologies based on graphs as well as the addition of firewall rules and the possibility of dynamically moving network nodes. This section is expected to take a long time to get done given the breadth of its scope. We have decided to tackle all these additional features only after getting an initial topology up and running because they do not belong to the network level. By this we mean that the ability of moving nodes or the preprocessing of graphs are not concerned with the instantiation of the virtual network. We first need a working network which is the heart of the project to then add functionality layers over it. Hence, these tasks are deeply related and yet conceptually different. That is why we thought it wise to clearly separate them. This section will be considered finished when we manage to virtualize the test network included in \cite{bib:ICS}.
            \item \textbf{Proof of concept}: With a fully functional testbed, we'll develop a ``\textit{malware}'' sample that propagates throughout the network. We'll then release it to analyze it's behaviour as well as the impact altering the network topology as this attack tries to progress.
            \item \textbf{Drawing conclusions}: After getting all the work underway we'll reflect on what we have discovered, the challenges we faced and how we overcame them to provide a closing section for our work.
        \end{enumerate}

    \section{Working gear}
        We have tried to rely on as little technologies as we could. We do depend on the Linux kernel nonetheless, which means that our platform can only be run on Linux based systems. We will out the development on a machine running \texttt{Ubuntu 18.04} but this framework should run on any Linux based distribution that's got a copy of the \texttt{iproute2} suite. We intend on running our work on commonly used distributions such as \texttt{Manjaro}, \texttt{Debian}, \texttt{Fedora}...\\

        Our control program will be written in \texttt{Python 3}. This language is platform independent in the sense that it relies on an interpreter thats available for the major operating systems. We'll also use the \texttt{networkx} library for handling the graphs describing the network topologies to virtualize. We have deliberately tried to minimize the number of external dependencies so that the largest possible part of the code is our own. This can be clearly seen on the way we log the program's operation and how we provide colours for example: it's all handled with \texttt{print()} statements and \texttt{ANSI} escape sequences for instance.\\

        As explained before, we'll employ \textit{Docker} for running the containers that implement both the hosts and routers in the network. We'll control the containers currently running from our management program through Docker's own command line interface so that we don't have to deal with the Python API for Docker, thus reducing the number of external dependencies. These containers rely on the \texttt{SSH} daemon to keep in running, that is, we'll also employ \texttt{SSh} in a recurring manner.\\

        Networking infrastructure other than hosts and routers will be provided by the Linux kernel and we'll control it through \texttt{iproute2}'s command line interface from our management program as well.\\

        Interaction with the running program as well as with the virtualized hosts and routers will be carried out from a command line. We have employed both \texttt{bash} and \texttt{zsh} for this purpose.\\

        Development will be carried out with the \textit{Visual Studio Code} editor and we'll use \textit{git} as a version control system (VCS). Instead of relying on graphical interfaces for managing our work we'll exclusively use git's command line interface. The projects remote repository will be hosted in a machine belonging to the laboratory we'll be developing the project on.\\

    \bibliographystyle{plain}
    \bibliography{refs}{}
\end{document}
