\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{
    a4paper,
    left = 18mm,
    right = 18mm,
    top = 14mm,
}

\usepackage{filecontents}
\begin{filecontents}{refs.bib}
@article{bib:REACT,
    title = "REACT: reactive resilience for critical infrastructures using graph-coloring techniques",
    journal = "Journal of Network and Computer Applications",
    volume = "145",
    pages = "102402",
    year = "2019",
    issn = "1084-8045",
    doi = "https://doi.org/10.1016/j.jnca.2019.07.003",
    url = "http://www.sciencedirect.com/science/article/pii/S1084804519302279",
    author = "Ivan Marsa-Maestre and Jose Manuel Gimenez-Guzman and David Orden and Enrique {de la Hoz} and Mark Klein",
    keywords = "Network security, Network theory (graphs), Optimization, Simulated annealing",
    abstract = "Nowadays society is more and more dependent on critical infrastructures. Critical network infrastructures (CNI) are communication networks whose disruption can create a severe impact. In this paper we propose REACT, a distributed framework for reactive network resilience, which allows networks to reconfigure themselves in the event of a security incidents so that the risk of further damage is mitigated. Our framework takes advantage of a risk model based on multilayer networks, as well as a graph-coloring problem conversion, to identify new, more resilient configurations for networks in the event of an attack. We propose two different solution approaches, and evaluate them from two different perspectives, with a number of centralized optimization techniques. Experiments show that our approaches outperform the reference approaches in terms of risk mitigation and performance."
}
@article{bib:ICS,
    title={Improving ICS Cyber Resilience through Optimal Diversification of Network Resources}, 
    author={Tingting Li and Cheng Feng and Chris Hankin},
    year={2019},
    eprint={1811.00142},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}
\end{filecontents}

\usepackage{cite}

\title{\vspace{-1cm}Undergraduate Thesis Proposal}
\author{Pablo Collado Soto \\ \\ \textbf{Universidad de Alcalá}}
\date{}

\begin{document}
    \maketitle

    \section{Introduction}
        The object of the proposed Undergraduate Thesis sits within the scope of the \textit{CloudWall} project (\texttt{PID2019-104855RB-I00}) from the \textit{Automatics Department} at the \textit{University of Alcalá}. The project's main focus is to develop a Cloud-enabled Resilience Framework
        tailored for the needs of the healthcare IT infrastructures to increase their capability to prevent and react to cyber attacks.\\

        In other words, \textit{ClodWall} would confer a network the ability of reconfiguring itself in the face of an attack so that it poses a higher degree of resistance to the attack itself. This reconfiguration implies a tolerable degradation of the service offered by the network which we can safely assume provided the network's resistance is indeed enhanced. The experiments carried out on a mathematical models of the networks have shown a better risk reduction and performance than the reference implementations as seen in \cite{bib:REACT}.\\

    \section{Objectives and field of application}
        We have already described the project our Undergraduate Thesis belongs to. Now, the objective we pursue is the full development of a virtualized network infrastructure that can be used as a testbed.\\

        Throughout the development process we will try to make our design as modular as possible. Even though the main use of our platform will be testing network resilience strategies, we believe its applications might be further reaching. At the end of the day, our work might be considered like a ``black box'' that takes a graph describing a network as an input and then generates a virtualized network that reproduces that description. This can then be used for purposes as diverse as testing network protocol performance on networks with an arbitrary topology, or as a learning environment for networking students.\\

        Given the flexibility of the tool we are to develop, its virtual use cases are as numerous as we can seemingly think of.\\

    \section{Project description}
        At its heart, a network is nothing more than a collection of hosts or end-systems interconnected by several packet forwarders. These packet forwarders are usually referred to as \textit{routers} when they deal with \textit{IP datagrams} at the \textit{network} layer, or as \textit{switches} when they deal with \textit{frames} at the \textit{link} layer. Then, our task is to virtualize these three types of nodes and then interconnect them according to the topology described by a user through an input graph.\\

        In the age of networked services we have seen technologies such as \textit{containers} rise. One of the best known implementations of this container technology is \textit{Docker}. We therefore chose to model our hosts and routers as simple docker containers running an image of the ever pervasive \textit{Ubuntu} operating system. The choice of containers over full-fledged virtual machines is not an arbitrary one for virtualizing nodes of a network. Whilst a virtual machine chooses to run a whole \textit{kernel} on top of the one supporting the host machine, containers leverage the preexisting kernel and only isolate the process we run within them. We can then consider virtual machines to work at the operating system level whilst a container operates at the process level. The difference in approaches makes containers a fairly low weight option as opposed to virtual machines. Hence, using containers as hosts and routers makes our virtual networks that much more scalable as we won't be needing as many resources when the number of nodes begins to increase. What's more, a container runs a \textit{container image} through which the user can define what he or she wants the container to run. This makes our approach tremendously flexible as the hosts and routers need not only run traditional operating systems: they can perform however the end user likes.\\

        Our project will leverage the flexibility of the \textit{Linux} network stack for getting much of the network infrastructure up and running. This will allow us to virtualize networking infrastructure such as point-to-point links and link layer switches so that we can use them as building blocks. This approach that relies on manually generating all the virtual network infrastructure can seem a little bit over the top. With solutions for making containers work together such as \textit{Kubernetes}, we might think it better to base our work on them. The problem is that these existing programs assume our objective is providing a service in a scalable and robust manner. Our objective has nothing to do with that goal, and so we have decided to build everything from the ground up instead of ``fighting'' against a preexisting system.\\

        We now know how we'll instantiate the different nodes we might need, but we still need to take care of two aspects. First of all, we need to somehow orchestrate the instantiation process so that we don't end up trying to connect two nodes that don't yet ``exist'' for example. We will also have to \textit{parse} the network description provided as a graph to make sense out of it and then proceed to network instantiation. We shan't forget that the aim of this infrastructure is providing a testbed for analyzing the performance of the techniques put forward in \cite{bib:REACT}. This implies that the network will need to reconfigure itself which ultimately means we will need to move nodes around. These dynamic changes will entail changes in the network configuration: forwarding tables in routers will need tweaking and \texttt{IP} addressing will need to be revisited, for instance. Being able to respond to these requirements means we need to keep track of some sort of network state. This aspect is something we need to take into account when designing our software architecture: we need to dynamically change the network, it's not only a matter of just bringing everything up.\\

        In an effort to simplify the network's operation we decided not to run any ``non-essential'' algorithms. This includes routing algorithms such as \texttt{OSPF} and automatic configuration protocols like \texttt{DHCP}. Going without these elements will force us to keep track of the network's state, which we expect to be a challenge.\\

        All in all, we will try to generate a framework for virtualizing arbitrarily complex network topologies. In order to meet the requirements imposed by the virtualized infrastructure's main objective we will have to add a \textit{control layer} to keep track of the network's state so that we can dynamically alter its topology. Having extensibility and flexibility in mind at all times, we are giving the user the freedom to choose whatever container image they want to run as hosts so that the topology can be used for whatever means he or she deems necessary. We then conclude we will generate a system that, even though it's designed for a particular purpose, can accommodate very different use cases given it's great flexibility.\\

    \section{Development plan}
        The task at hand is not a small one. In order to maximize our chances of success we have broken down the development process into smaller pieces we can safely tackle:

        \begin{enumerate}
            \item \textbf{Getting to know the technologies}: As seen on the project description, we need to orchestrate several technologies together to get a working virtual network. Given our goal, we'll need to bend the technologies we are to use to their very limits to pull off what we are trying to achieve. This implies we'll need to have a very clear picture of the capabilities of each of them and how we can coordinate them to make their interactions as seamless as possible. This will require us to get acquainted with each of them through their documentation as well as trough small scale tests to assess whether we can used them in the final solution.

            \item \textbf{Designing the system}: Once we have a clear idea of the technologies to use we need to develop a software solution that will automatically take care of virtualizing a network. In a first approach we won't be using graphs to describe the network so as to simplify the first iteration of our solution. We'll then try to generate a very simple network to assess whether we are on the right track. This step will lay the foundations for a management system that will be able to cope with ever more complex topologies. We expect to have to revisit the previous step during the development of this first simple topology as we are certain we'll stumble upon obstacles we hadn't foreseen before starting the design.

            \item \textbf{Enhancing the design}: We are certain that the addition of new functionalities on top of those defined for the first approach will raise unforeseen problems. Solving these will require rethinking parts of the initial design so that we can accommodate the code structure to the needs of the virtual network. We'll tackle more complex topologies in this section as well as the ability of defining network topologies based on graphs. We'll also work on the addition of firewall rules and the possibility of dynamically moving network nodes. This section is expected to take a long time to get done given the breadth of its scope. We have decided to tackle all these additional features only after getting an initial topology up and running because they do not belong to the network level. By this we mean that the ability of moving nodes or the preprocessing of graphs are not concerned with the instantiation of the virtual network. We first need a working network, which is at the heart of the project, to then add functionality layers over it. Hence, these tasks are deeply related and yet conceptually different. That is why we thought it wise to clearly separate them. This section will be considered finished when we manage to virtualize the test network included in \cite{bib:ICS}.

            \item \textbf{Proof of concept}: With a fully functional testbed, we'll develop a ``\textit{malware}'' sample that propagates throughout the network. We'll then release it to analyze it's behaviour, as well as the impact altering the network topology has on the attack as it tries to progress.

            \item \textbf{Drawing conclusions}: After getting all the work finished we'll reflect on what we have discovered, the challenges we faced and how we overcame them to provide a closing section for our project.
        \end{enumerate}

    \section{Working resources}
        We will try to rely on as little technologies as we could. We do depend on the Linux kernel nonetheless, which means that our platform can only be run on Linux based systems. We will carry out the development on a machine running \texttt{Ubuntu 18.04}, but this framework should run on any Linux based distribution that's got a copy of the \texttt{iproute2} suite. We intend on running our work on commonly used distributions such as \texttt{Manjaro}, \texttt{Debian} or \texttt{Fedora} to make sure our work is as multi-platform as we expect it to be.\\

        Our control program will be written in \texttt{Python 3}. This language is platform independent in the sense that it relies on an interpreter thats available for the major operating systems. We'll also use the \texttt{networkx} library for handling the graphs describing the network topologies to virtualize. We have deliberately tried to minimize the number of external dependencies so that the largest possible part of the code is our own. This can be clearly seen on the way we log the program's operation and how we provide colours: it's all handled with \texttt{print()} statements and \texttt{ANSI} escape sequences, for instance.\\

        As explained before, we'll use \textit{Docker} for running the containers that implement both the hosts and routers in the network. We'll control the containers currently running from our management program through Docker's own command line interface so that we don't have to deal with the Python API for Docker, thus reducing the number of external dependencies. These containers rely on the \texttt{SSH} daemon to keep in running, that is, we'll also use \texttt{SSH} in a recurring manner.\\

        Networking infrastructure other than hosts and routers will be provided by the Linux kernel and we'll control it through \texttt{iproute2}'s command line interface from our management program as well.\\

        Interaction with the running program as well as with the virtualized hosts and routers will be carried out from a command line. We have used both the \texttt{bash} and \texttt{zsh} shells for this purpose.\\

        Development will be carried out with the \textit{Visual Studio Code} editor and we'll use \texttt{git} as a version control system (VCS). Instead of relying on graphical interfaces for managing our work we'll exclusively use git's command line interface. The projects remote repository will be hosted on a machine within to the laboratory we'll be developing the project on.\\

    \bibliographystyle{plain}
    \bibliography{refs}{}
\end{document}
