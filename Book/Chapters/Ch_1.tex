\chapter{Introductory Theory}
    \section{Project's Background}
        Our work cannot be understood if we don't take into account our tutor's research group's current line of research. That is no other than \textbf{network resilience} and how we can improve it. Their approach is broadly based on modeling a network infrastructure as a multilayered construct where the upper layers get closer and closer to reality as we continue climbing them down. We can see how this approach is quite similar to that of conceptual network stacks such as the ones running today's Internet.\\

        The above line of work has produced some high-quality papers worth of theory. We mustn't forget that we are engineers nonetheless, which means we ought to look for a real-world application for our solutions. In an effort to somehow experimentally measure the effectiveness of the defense strategies proposed by the group's research we have been tasked with the development of a testing \textit{framework}. Said \textit{framework} needs to somehow \textit{emulate} an arbitrary network topology on which we can operate in such a way that we can mimic real world attacks. By applying the researched mitigation strategies on said scenario we plan on being able to settle which do best based on the current threat and network topology.\\

        \subsection{Simulation vs Emulation}
            These two terms are often used interchangeably when referring to tools whose mission is providing the user with a scenario resembling the real world in some sort of way. Even though both terms share the same purpose the way in which the accomplish it is radically different.\\

            \textbf{Simulation} leverages the theory capable of modeling real world phenomena. The clearest example may be physics. Physics let us model the world that surrounds us through math. That is why we can leverage the pertaining equations to compute the outcome of any scenario we can describe. Thus, Simulation \textbf{computes} an outcome based on the initial parameters and the model we have built for describing the system under study. This implies that simulation is limited by how good our models are. If an equation doesn't take an aspect into account that means it won't affect the simulation's outcome which in turn can result in inaccurate results. Techniques used for simulating systems include discrete event and agent based simulations such as those used by the well known \texttt{Any Logic}.\\

            \textbf{Emulation}, on the other hand, tries to recreate the system under study to then perform experiments on it. If we manage to craft a detailed enough model, we could even get a glimpse of unexpected behaviors we hadn't taken into account. This is why emulation is often the preferred approach. Nonetheless, it is harder to recreate a system than it is to describe its expected behavior. In our project we have nonetheless decided to go with emulation so that we reaped the most useful information from our experiments.\\

    % TODO: Add images from: https://www.docker.com/resources/what-container
    \section{Outlining the Implementation}
        \subsection{Getting the "metal"}
            Given our objective we need to come up with a way of emulating a whole network. Before settling on what software platform (i.e operating system) we are going to employ we need to decide which technology is going to provide the machines we run those operating systems on. These machines need not be "real" as we can access the ever pervasive virtualization technology as well as more modern approaches such as containers. We'll devote the next section to discussing the pros and cons of each alternative so as to reach a final decision on what technology to use.\\

            The initial impulse we had was to leverage the current virtualization capabilities offered by tools such as \texttt{VirtualBox} or \texttt{VMWare}. We were also aware of other existing solutions such as the \textbf{container} technology offered by \texttt{docker} for instance. What's more, we also knew of the existence of orchestration tools such as \texttt{kubernetes} which were aimed at employing several containers in a cooperative and organized manner. Given we need to build a network, no matter what technology we end up using, one may believe \texttt{kubernetes} to be quite a handy addition to our tool belt. We'll walk through what each of the above offer and how they accomplish their goals in an effort to decide which of them to employ as the cornerstone for our work.

            \subsubsection{Virtual Machines (VMs)}
                In today's world, technology firms tend to lock their products up and make them incompatible with other industry solutions in an effort to bite the biggest possible chunk out of the possible user base for their assets. This has left many end users having to cope with running several operating systems (OSs) on a single machine so that we had access to particular programs such as the \texttt{COMNET III} GSM Network Simulator for example.\\

                Odds are that in order to have several operating systems coexisting we have turned to the power offered by Virtual Machines. Put simply, VMs \textit{emulate} a whole \texttt{guest} operating system within a \texttt{host} OS. In order to do so, virtualization solutions like the ones we mentioned before leverage the capabilities of a "middleman" known as the \texttt{hypervisor}. This \texttt{hypervisor} may be implemented in hardware or software and it provides an interface letting \texttt{guest} operating systems share the available computing resources with the \texttt{host} OS.\\

                Even though we find the above topic extremely interesting we are not concerned with aspects such as the amount of CPU time or memory that would be devoted to our VM. We, on the contrary, are mostly concerned with what happens with the network infrastructure. After all, our aim is creating a virtualized network and so we need to know how the virtual network attached to the created VMs is set up. We know that the VMs actually have Internet connectivity through the \texttt{host} OS so there must be some sort of "infrastructure" supporting said connections. Nonetheless and before we consider VMs as a feasible solution we need to think about how the'll scale.\\

                The network topologies we have been charged with virtualizing are not small. Our largest working topology consists of $45$ nodes, of which $36$ would need to be implemented as a VM instance. Given how resource intensive VMs are when compared to other solutions like containers these numbers are too large to be handled in terms of virtual machines. If we just consider the amount of memory we would need to devote for them on our $8\ GB\rvert_{RAM}$ machine we would be looking at roughly $234\ MB\rvert_{RAM}$ for each of them and even then we would have no memory for the \texttt{host} OS at all. On top of that, the image for \texttt{Ubuntu 20.04} weighs $958,4\ MB$. It would be cumbersome to get it below that threshold as we would need to strip the \textit{vanilla} version of any features we don't need and we would then have to re-package the result. Even then, we would be looking at around $72\ GB$ of used \texttt{HDD} space if we were to allocate $2\ GB$ of \texttt{HDD} space to each and every VM we were to bring up.\\

                Even though the above requirements could be eventually met it's easy to see how this approach wouldn't scale much further than it already has if running it on consumer grade platforms. We can then conclude how the fact that VMs are just "whole" operating systems makes them too cumbersome to handle and too "big" to be instantiated all the times we need them. On top of that and even though we didn't dig that deep into the underlying network infrastructure, it seems to be "darker" and less documented than that offered by others like \texttt{docker}. Then, although VMs are a perfect fit for many scenarios that is not our case. Knowing what set us back we'll now analyze the \texttt{container} technology to find out how it's a perfect way to get started for us.

            \subsubsection{Containers}
                Before beginning to discuss whether containers are suitable for our purpose we should begin by describing what a \texttt{container} really is as it can be a confusing actor in the virtualization realm.\\

                According to \href{https://www.docker.com/resources/what-container}{docker's documentation}, \textit{a container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another}. Now, this definition is a perfect representative of the main kind of problems we'll have to face when trying to bend containers to our will throughout the development. Containers are born to try and increase the portability of applications. In todays Internet-centric society where we all want uninterrupted services it's critical to be able to change an applications environment at a moment's notice due to outrages, ending support cycles for software, security breaches... That's why the container technology has developed with such an enormous speed. It provides the infrastructure for a reliable service delivery. Now, we could "twist" de definition if we were to think of applications as full-fledged operating systems. That is, if we run a whole OS within this containers we would be actually achieving our goal: each container will behave as a network node so, in other words, we would only need to have a flimsy $36$ containers up at the same time; piece of cake.\\

                One of the questions we asked ourselves when trying to decide on a technology was: what makes a container different than a VM? If we go back to the section we devoted to virtual machines we'll notice how they run a full-fledged operating system as a guest. This implies each VM has its own \textit{kernel}.\\

                We don't need to dive that deep into what a kernel is as that would be going down the OS-side rabbit hole. We just need to know that the kernel is the piece of software "gluing" the hardware and user-land software (programs such as browsers) together. That is, it allows applications to access the computing resources in an organized manner enabling the sharing of resources amongst them. As we are telematics engineers we usually find the kernel concept easier to understand in terms of the abstractions it offers use; the network socket being the most familiar. Through it, our applications can leverage the networking capabilities of the machine they're running on.\\

                Unlike VMs, containers all share a single kernel. We can then think of containers as a way of isolating applications along with all their dependencies within a shared computing platform. We don't need "fancy" tools such as an hypervisor; we would only need to be very meticulous and know the kernel's offered facilities very well to achieve the end result offered by containers. Projects like \href{https://github.com/p8952/bocker}{bocker} prove one can achieve similar results to the ones provided by docker if only interested in a subset of the latter's capabilities. Nonetheless, we'll think of containers as light VMs throughout the development as, even though it's not exactly true, it won't hinder our development approach.\\

                \paragraph{Containers and Docker}
                    Once new terms start to be thrown around it's easy to get confused and that was exactly the case for us. One point we weren't that clear on was the difference between docker and containers. Are they the same thing?

                    It turns out that the container technology is somewhat an standard. Now, the way that technology is implemented can vary. That's where docker comes into the picture as it offers an \textit{implementation} for containers which is loosely called the \texttt{docker engine}. This is a similar situation to that of VMs. The idea of a virtual machine has found two main implementations by VirtualBox and VMWare. Taking the network view of things it's like what happens with RFCs by the IETF. They propose a standard and several people try to implement it according to their coding style, level ok knowledge...

                    If we are to be entirely correct when it comes to nomenclature we would have to say that our solution is going to be based on docker's container implementation.\\

                On top of container technology being lighter than that of VMs, which makes it way more scalable, we also found the amount of documentation regarding the network infrastructure to be that much larger. That gave us a strong foothold in our path to getting our framework up and running. Another positive aspect on containers we initially considered was the existence of \textit{container orchestration tools} such as \texttt{kubernetes} which we thought could make our job easier. We'll devote a few paragraphs to discussing why in the end \texttt{kubernetes} wasn't that much of a fit for us. All in all, we'll include an in-depth analysis about docker containers later down the road.

            \subsubsection{Kubernetes}
                As stated in Kubernetes' own \href{https://kubernetes.io/docs/home/}{documentation}, \textit{Kubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications}. In other words, \texttt{kubernetes} let's us define how we want our application to behave through a manifest provided in a \texttt{*.yaml} formatted file. Now, if we considered docker to be focused on the "service" aspect of things in that it aimed at offering robust services, Kubernetes takes it to the next level. It's a product aimed at professionals that's engineered to be stable. That means that meddling with the internals in an effort to achieve our goals was bound to be an extremely difficult task. By employing kubernetes we would be using an infrastructure that's already been laid for us, which is difficult to change and that does not behave how we want it to. We believe using a tool only to work against its basic principles is a wrong approach. That's why we decided to manually build the needed infrastructure from the ground up departing from docker containers.\\

                Just to give a concrete example of what we mean by working against the pre-existing infrastructure we could take a look at how kubernetes connects nodes. As it needs to offer some kind of service it will set up the internal network topology in such a way that all the nodes are able to connect among themselves. Given our requirement of implementing a firewall in the topologies we need to be working with we can conclude it would be insane to try to implement a firewall in an infrastructure whose primary concern is enabling communication links amongst all the network nodes. It's this diametrically opposite approach that made us dismiss kubernetes as a feasible solution.\\

            With the above discussion in mind we'll take a look at how we'll leverage the features docker provides for us and talk about how we are specifically take advantage of them.

        \subsection{Getting the OS}
            After settling on an option providing the "hardware" our software is to run on we need to decide which operating system is up for the task we have been proposed. Given today's ecosystem we quickly thought of the three main contenders in the user market. These were none others than \textit{Microsoft Windows}, \textit{macOS} and \textit{Linux}. Before delving any deeper into the discussion about which to employ we would like to clarify our choice of words when regarding the penguin's OS.\\

            \paragraph{Battle of the Names}
                Eminences such as Richard Stallman would argue that not referring to a Linux-based distribution as a GNU/Linux one would be a hate crime. Even though we are sure the reader will be aware of the subtleties of the nomenclature with this type of OS, we would like to discuss its origin and why we chose to refer to them as just Linux systems.

                Strictly speaking, Linux is just the kernel of Linux-based distributions or distros. As we explained in the previous section, a kernel is the piece of software gluing the hardware and user software together. It accomplishes this non-trivial feat by offering applications an interface through which they can request services to the kernel. This kernel will then fulfill these requests in an orderly manner so that applications can coexist and cooperate towards a better usage of the system overall. These applications can in fact not even be aware that they are running alongside others.

                The kernel is for many the most crucial piece of software within a full fledged operating system. It's the cornerstone on which everything else is built. Nonetheless, if a non-technical user were given just a kernel they would have a very hard time making any use of it. This is where applications or user programs come into play. The make use of the OS's services and they let an end-user get meaningful work done. These end user programs range form text editors such as \texttt{vim} or \texttt{Microsoft Word} to VoIP (Voice over IP) PBXs (Private Branch eXchanges) like \texttt{asterisk}.

                The role of GNU in all this is providing many of these end user programs as free (as in freedom) software. Huge projects like the \texttt{gnome} desktop environment which may detest (even though we like it) and \texttt{make} (which we are using for compiling this \LaTeX file) carry the GNU stamp on them. Given the above, GNU argues operating systems packaging their tools should be considered GNU/Linux systems as these two terms work cooperatively, i.e. they are software pieces with distinct purposes. While we consider this to be absolutely true for end-user systems such as \texttt{Ubuntu desktop} and \texttt{Debian} we feel this is not the real case with us.

                We will be running our network nodes as docker containers as we'll see in just a moment. The thing is, we'll try to make the image ran by these containers as lightweight as possible. In doing that we will only rely on GNU's shell, \texttt{bash}, for running some commands which aren't doing the heavy lifting at all. One can confirm this statement by cross checking our used software tools with \href{https://www.gnu.org/software/software.html}{GNU's Software List} That is why we will refer to our platform as Linux containers instead of GNU/Linux ones.

            Knowing we have already restricted ourselves to the use of container technology we can discard \textit{macOS} as an option right away as there is no official support for it. We could have used \textit{Windows}-based containers but given our uncommon requirements we opted to use a \textit{linux} based one. Given its current pervasiveness we decided to use \textit{Ubuntu} docker images for running our nodes. The version we employed during the testing phase was deemed \texttt{Bionic Beaver} and it was numbered \texttt{18.04 LTS} (Long Term Release) which boasts a higher stability than yearly versions. Given our requirements all our work should run just fine on newer versions. We nonetheless recommend sticking to \texttt{LTS} releases as the yearly builds can mess some internals up in the name of progress... On top of its flexibility we mustn't forget about the price and licensing factors. Even though we haven't looked into it regarding Microsoft's OS, Windows licenses tend to be more restrictive than those attached to Ubuntu. As we plan on messing around with the kernel's internal network infrastructure we aren't entirely sure if that would be allowed by window's license terms... Add that to the fact that Ubuntu doesn't cost a cent and we have got ourselves a clear winner.\\

            Knowing the technology and operating system in charge of providing our virtual network's backbone we'll try and delve deeper into how can Linux provide us with the flexibility our task is desperately calling for.