\chapter{Used Technology Analysis}
    According to the discussion in the previous chapter we have settled on docker containers running Ubuntu for providing the backbone of our virtualized networks. Now, once we are clear so as to what technology to employ we need to get down to the nitty gritty of implementing a full-fledged virtual network infrastructure. In order to do so we need to become acquainted with the Linux kernel.

    \section{The Network Stack}
        A fascinating but rather messy image is \href{https://upload.wikimedia.org/wikipedia/commons/5/5b/Linux_kernel_map.png}{Linux's "Map"}. If we pay close attention we'll see how one of the columns is just devoted to networking. The software entities comprising this column is what we will refer to as \textit{Linux's Network Stack}\\

         The word stack is something that shows up time and time again in the area of networking. It helps us have a top-level view of how logical entities cooperate within a network. When we think about stacks we naturally begin to consider them in terms of the layers they are composed by, with each layer tackling a simple task and offering services to the layer above whilst using those provided by the layer below. It's not going to be any different with Linux; we can think of its network stack as a huge "blob" of code which all network packets reaching a Linux-based system traverse. Thus, if we can alter how Linux processes packets or build "virtual" connections between different network stacks we would be capable of constructing a de-facto virtual network tailored to our needs.\\

         \paragraph{Naming Packets}
            Before we go on we need to shed some light on the naming we are going to use regarding the units of data exchanged through network links. Even though the term packet is tremendously generic we feel it's not a wrong one to turn to in our case. When we wire up several network nodes together we are looking for full connectivity, that is, connectivity at the application level. Thus, we are not really that interested on what layer the "packet" is at, we don't really care if the packet is a \texttt{segment}, \texttt{datagram} or \texttt{frame}. In the case a need for more specific naming turns up we won't hesitate to use it but we prefer to keep the writing simple and avoiding getting bogged down with technicalities where we feel the benefit is not that obvious.

            A prime example of the above would be the use of the term \texttt{packet} instead of \texttt{link-layer frame} in the section's introduction. \texttt{Frame} is the correct term for referring to the data structure a \texttt{NIC} (Network Interface Card) hands to the kernel (albeit somewhat processed as the preamble and Frame Check Sequence of Ethernet frames are usually stripped from incoming frames by the \texttt{NIC} itself as seen \href{https://gitlab.com/wireshark/wireshark/-/wikis/Ethernet}{here}).

        If we think about network stacks, we would probably believe we need to have one per machine. That is, every network-capable devices must have their own \textit{data-path} which packets are to traverse. What may not be so simple is thinking that a machine \textit{may} have more than one network stack. In order to get a firmer grasp on the implications of the above idea let's revisit the world of network interfaces.\\

        When we first started learning about network architectures we where boggled by the fact that a machine could have more than one NIC. This implied it had several different IP addresses "attached" to it which provided redundancy amongst other several capabilities like traffic control. What astonished us the most was the raw power of not imposing a limit on the number of NICs a machine could have. That simple fact allowed routers to exist for instance and you could do "silly" stuff such as getting a packet through an interface and \texttt{echoing} it out the other. All in all, it provided loads of flexibility to the whole system.\\

        Now, if we transpose the above to the concept of network stacks we could be talking about packets being interchanged in between them whilst residing on the same machine nonetheless. If we sit back and take a look at the larger picture we can clearly see that packet exchanges below the application level are logically switched between network stacks belonging to different machines as it's the stack who's in charge of processing said data structures. Seeing matters in that light and knowing we can have several network stacks on a single platform, talking about packets being interchanged within a machine doesn't seem that far fetched now.\\

        Given the previous discussion we can now clearly see the base on which everything else is built upon. The ability to have several coexisting network stacks on a machine and being able to connect them as we please is such a powerful tool that our work is only scratching the surface of the capabilities enabled by this kind of technology. Linux's network stacks are nothing short of an ode to code modularity if you ask us.\\

        All the previous discussion is related with the theoretical or conceptual realm of matters. We will now delve into how we can translate these ideas into a working virtual network. After going through that process we'll also look into how we decided to do everything manually once we have a broader technical background on the subject.\\

        Finally, we feel we need to clarify that, even though it may be clear at this point, all the network traffic we are to generate originates from within our own system. In other words, our network would be perfectly capable of working without any Internet access.\\

    \section{The Network Namespace}
        % \pyfile{Code_snippets/foo.py}

        % Just as we have previously discussed, design and implementation are two very different things. A prime example is the case of the \texttt{SNMP} tools where \texttt{SNMP} stands for \texttt{Simple Network Management Protocol}. This protocol was originally defined in \href{https://tools.ietf.org/html/rfc1067}{RFC 1067} but no implementation was defined. With time, the \texttt{net-snmp} suite of tools became the de-facto implementation for \texttt{SNMP}. It is then of the utmost importance to notice that the design and implementation needn't be carried out by the same people; in fact, it's not uncommon to have several implementations for a given design. That's the case with shells of which we have so many, \texttt{bash}, \texttt{zsh}, \texttt{ksh} and so on. Then, we can expect the Linux...\\

        Many of the programming languages dominating today's market are object oriented. This, very roughly, means that the programmer is expected to generate \textit{classes} representing "real-life" entities to some extent. These classes are defined by their \textit{attributes} (characteristics) and their \textit{methods} (what they can do). Now, a class by itself cannot do anything, we need to \textit{instantiate} it so that we create an \textit{object} of that class. We'll then be capable of using the newly created object as we please.\\

        This concept of instantiation is actually quite powerful as it appears continuously in many areas of engineering. Now, we could say that Linux's network namespaces work in a similar fashion to classes. We can think of the network stack as the class and what we call \textit{network namespaces} as the objects.\\

        As seen \href{https://en.wikipedia.org/wiki/Linux_namespaces}{here}, namespaces are not only seen when doing networking stuff, they are a feature of the Linux kernel employed in many other areas. These namespaces let us partition kernel resources so that each process sees a resource that's only for it, it's not shared. When applied to networking we can see how each network namespace represents an entire network stack. Then, if we set up several network namespaces (namespaces from now on as this is the only type of namespace we will deal with) we have effectively housed several network stacks under the same roof. If there was any way of interconnecting these namespaces we would have taken quite a dent off our task.\\

        Now, our system will model a network-capable machine as a simple process with its very own network namespace. This sentence can seem "loopy" at best, so let's think about a more down-to-earth scenario through which to understand it better.

        \subsection{A common \texttt{HTTP} server}
            The \texttt{Internet} has revolutionized society in ways little people could have predicted. It supports many different application protocols such as \texttt{SMTP} and \texttt{FTP}, but one of the most popular (if not the most) is \texttt{HTTP(S)}. This protocol provides the backbone for websites and many other applications that are employed by huge amounts of people on a daily basis.\\

            Like many other protocols, \texttt{HTTP(S)} leverages the \textit{client-server} architecture. The client role is usually "played" by web browsers such as \texttt{Firefox} or \texttt{Safari}, whilst the server part is relegated to programs such as \texttt{Apache} and \texttt{Nginx}. From a networking point of view, a process is univocally addressed through an \texttt{IP address} and a \texttt{port number}. The former identifies a given machine within a network whilst the latter targets a process running within that machine. Then, we can have different servers running on a \textit{physical} machine as long as their port numbers differ. We begin to see how, from a strict networking point of view, we can have several \textit{logical} entities supported by a single \textit{physical} one.\\

            Now, imagine that we want or need to run two \texttt{HTTP(S)} servers on a single machine and a single port number. That wouldn't be possible, would it? Now, if we leverage the concept of the network namespace we can circumvent this limitation. We could, for example, instantiate two different namespaces for each of the server processes and then bind each of them to the same port \textbf{within the respective namespaces}. Then, a network-aware process together with its own network namespace is, in a way, a full-fledged logical entity within a network: it's addressed by its own \texttt{IP address} and it has it's own pool of independent \texttt{port numbers}. This is exactly what our framework exploits: each virtual host is a simple process running within its very own namespace.\\

        The above was one of the main reasons we vouched for \textit{containers} as a technology before. Each container ships with its own network namespace when it comes to networking. Whilst it's true that \textit{docker} does bring up some network infrastructure with a new container, we can explicitly avoid that to be given a "pure" namespace with each new container. We can then manually instantiate any other network elements we might need to complete the network topology we are to build.\\

        Managing namespaces and instantiating the virtual network elements, such as \texttt{veths}, gluing them together is attainable with the help of the \texttt{iproute2} suite of tools. We'll then look into how to leverage \texttt{iproute2} for our needs.\\

    \section{The \texttt{iproute2} suite}
        The time we spend on terminal emulators has exponentially increased as we worked our way through our bachelor's. Even though they provide quite a fast way to "get around" the computer they can be a little abstract at times. We are always aware that the OS "under" us has many running processes and provides us with services that can be used at our request. The thing is, faith can at times be scarce so we usually have tools that can inform us about what's going on "behind the scenes". This is the case for programs like \texttt{ps} which reports the status of the system's processes or \texttt{w} which tells us about who is currently logged on the system as well as what they are up to. When using a shell we are aware of the fact that other processes are running and that other users may be logged in but we nonetheless have tools that let us consult the current status of the system.\\

        Where networking is concerned, \texttt{iproute2} is like a Swiss Army Knife. It's a \textit{suite} or collection of tools that lets us do everything from inspecting the current interfaces and routes to generating \texttt{IPv6/IPv4} tunnels. We'll only be concerned with the \texttt{ip} command in our case which is in charge of "showing/manipulating routing tables, network devices, interfaces and tunnels" as seen in \texttt{ip}'s manpage (which can be consulted with \texttt{man ip}). The uses one can give to \texttt{iproute2} are seemingly endless, but before getting into them we believe it would be useful to compare the \texttt{iproute2} suite to its predecessor: \texttt{net-tools}.\\

        \subsection{Describing a network interface}
            In a previous section we already provided some comments on what NICs are. Knowing that interfaces create "bridges" between different systems we can clearly see how these network interfaces bridge digital systems to a communication network; they let these digital machines leverage the communication capacities of computer networks.\\

            Whilst almost all engineers could recognize what a NIC physically is, the matter is not that simple regarding how the OS treats these interfaces. We find it quite helpful to think of what we would do if we had to implement some software solutions that had to employ network's capabilities. There must be a "way" that these hardware components can be used from the application perspective, that is, the OS needs to offer some kind of abstraction through which we can make use of the NIC itself. This abstraction is what we will call a OS-level NIC or network interface.\\

        \subsection{The deprecated solution: \texttt{ifconfig}}
            Querying a system's interfaces is a very common task in the day of a network engineer. Whenever internet connectivity is playing up or network connections don't seem to be working as intended, our initial step is to check interfaces are configured as they should. On \texttt{unix-based} systems such as \texttt{macOS} and \texttt{GUN/Linux} this was traditionally accomplished with the \texttt{ifconfig} command. Before \texttt{iproute2} was implemented, network-related tools fulfilling the same purpose were provided by the \texttt{net-tools} suite. Among the most well-known tools provided by it we can mention \texttt{ifconfig} and \texttt{netstat}. The former displayed information on the system's network interfaces whilst the latter shed light on the open sockets in the system. When working with network-centric applications such as web-servers, \texttt{netstat} was a superb way of finding out whether some process had \texttt{binded} (i.e. began listening) to the \texttt{80} or \texttt{443} ports, each being the default for the \texttt{HTTP} and \texttt{HTTPS} protocols, respectively. With time, \texttt{iproute2} became available and it \textit{deprecated} the \texttt{net-tools} suite. We have nonetheless observed throughout our studies that a non-negligible amount of people are reluctant to abandon \texttt{ifconfig} and related tools.

            What we are trying to achieve with this project is possible if leveraging \texttt{net-tools} instead of \texttt{iproute2}. However, the former's documentation and examples tend to be more cryptic and harder to digest. What's more, even though people resist to stop using it, \texttt{net-tools} is bound to disappear. That's why using the newer network tools confers a longer life prospect to our tool.

            Even though \texttt{net-tools} has been deprecated on \texttt{linux-based} systems, it's still the selected network tool suite for systems such as \texttt{macOS} where \texttt{iproute2} is \textbf{available}. Some programs such as \texttt{iproute2mac} do exist for this operating system, but they just \textit{parse} (i.e. process) the output provided by \texttt{net-tools} utilities and present it following \texttt{iproute2}'s format. We mustn't be deceived by looks: \texttt{iproute2} is a part of \textbf{\texttt{linux}} systems.

            We compare both suites on the table \ref{tab:net-tools-vs-iproute2} as provided by \href{https://www.thegeekdiary.com/comparing-net-tools-v-s-iproute-package-commands/}{this site}.

            \begin{table}
                \centering
                \begin{tabular}{|c|c|}
                    \hline
                    \textbf{net-tools} & \textbf{iproute2}\\
                    \hline
                    \texttt{arp} & \texttt{ip neigh}\\
                    \hline
                    \texttt{ifconfig} & \texttt{ip addr}, \texttt{ip link}, \texttt{ip tunnel}\\
                    \hline
                    \texttt{netstat} & \texttt{ss}, \texttt{ip maddr}\\
                    \hline
                    \texttt{route} & \texttt{ip route}\\
                    \hline
                \end{tabular}
                \caption{\texttt{net-tools} utils vs. \texttt{iproute2}'s}
                \label{tab:net-tools-vs-iproute2}
            \end{table}

        Knowing a bit about what \texttt{iproute2} is, we believe walking through some examples showing how to instantiate virtual network elements will prove rather revealing.

    \section{Instantiating virtual network elements}
        As \texttt{iproute2} offers us total control over a machine's internal network infrastructure, we believe the best way to showcase what we can do is through example. Before getting down to the "command level" of things, let's get some naming conventions out of the way.

        \subsection{Naming network elements}
            The networks we'll work with are rather simple when it comes to the type of elements they are composed of. Even though they can grow quite large, from a network architecture point of view they'll reuse the same components over and over. These components are:

            \begin{itemize}
                \item \textbf{\texttt{Link-level bridges}:} These \textit{layer 2} devices will forward \textit{link-layer frames} within a subnetwork based on the destination \texttt{MAC address}. These are \textbf{transparent} to \textit{layer 3} (i.e. \textit{network}) protocols such as \texttt{IP}. These are implemented as a \texttt{linux bridge} which, in turn, manifests itself as a network interface. They'll provide the backbone for each of the subnets we are to instantiate as they provide their very own broadcast domain. One can find documentation regarding these bridges \href{https://wiki.linuxfoundation.org/networking/bridge}{here}.

                \item \textbf{\texttt{Network-level routers} and \texttt{firewalls}:} These \textit{layer 3} devices will forward packets between subnetworks based on the destination \texttt{IP} address. These will be implemented as \texttt{containers} running a regular \texttt{ubuntu} image. Given these routers sit between subnetworks, we will also implement any required firewall functionality within them as well. We'll delve deeper into how this can be accomplished in due time, but we can say that these firewalls are based on \texttt{iptables}.

                \item \textbf{\texttt{End hosts}:} These are the end systems in the network. They'll be implemented as \texttt{docker containers} as well.

                \item \textbf{\texttt{Veths}:} These are \textit{virtual Ethernet interfaces} which we'll use to wire the entire network together. Note we had only described standalone elements up to now without any means of interconnecting them! These virtual interfaces can be regarded as "virtual wires" with two ends. Any frame coming into one end comes out the other and vice-versa. We can then conclude they behave exactly like real-world wires. We'll then "insert" one end of a \texttt{veth} into a network device and the other end into another one to effectively "wire them together".

                \item \textbf{\texttt{Network namespace}:} We would like to make it absolutely clear that \textit{namespaces} are \textbf{not} network devices per-se: they have no real world counterpart. As we'll later see, each \texttt{container} spawns its own \textit{namespace}: we have a one-to-one correspondence between hosts/routers and namespaces. This will be crucial when "wiring things up" with \texttt{veths}, as they'll have to be associated to a particular network namespace for things to work. We know some concepts might seem hazy at best for now: we promise things will clear up in due time.
            \end{itemize}

        Now that we have settled the naming scheme we'll use, it's time to break down some commands allowing us to instantiate virtual network elements as we please.

        \subsection{\texttt{Bridges}}
            Just like we stated before, \texttt{bridges} are \textit{layer 2} devices. We'll find how, due to their operation, no configuration is needed beyond bridge creation. These devices will automatically learn link-level routes as needed whilst in operation. As we are concerned with the link layer we'll be dealing with physical or \texttt{MAC} (\textbf{M}edia \textbf{A}ccess \textbf{C}ontrol) addresses. These are very characteristic in the sense that they are represented as a series of \textit{hexadecimal} digits separated in groups of $2$ by colons (\texttt{:}). Given the intricacies of a $base-16$ numbering system, each of these colon-delimited groups is equivalent to a single \textit{byte} of data. Anyhow, an example of a \texttt{MAC} address would be: \texttt{8c:2d:aa:56:e2:7b}. As a curiosity, we can indicate that different \texttt{MAC} prefixes are allocated to manufacturers by the \texttt{IEEE}. That means that we can know the manufacturer of a pice of equipment through its \texttt{MAC} address (assuming it's not been tampered with). If one were to check the above address, he or she would find it to belong to a device manufactured by \textit{Apple Inc.}. After all, it's the \texttt{MAC} of the \textit{iMac} (no pun intended) we are writing this document on.

            The bottom line to the above is that a bridge's \texttt{MAC} forwarding table need not be configured explicitly, be it through a protocol or a human operator. As soon as the bridge is brought up, it'll broadcast an incoming link-layer frame on all of its interfaces but the one the frame arrived on. The key aspect here is that the bridge will include an entry in its forwarding table associating this frame's \textbf{source MAC} to the interface the frame arrived on. If that original frame's \texttt{MAC} address was, say, \texttt{01:23:45:67:89:AB} and the frame arrived on port \texttt{0}, the bridge would automatically learn to send any incoming frame with destination \texttt{MAC 01:23:45:67:89:AB} through port \texttt{0}. After being in operation for some time, the bridge will throttle back the number of frames it broadcasts, thus operating ever more efficiently. An aspect to keep in mind with this way of doing things is that these entries need some way of being deleted. Just like with learning, this will be an implicit process requiring no explicit signaling whatsoever. The learnt entries will be associated to a timer. When this timer runs out, the route will be assumed to be stale and be deleted from the table. This mechanism allows us to remain true to the real network topology whilst being stable enough in the face of network changes.

            We believe the above discussion to be relevant as a way of justifying why we need't be concerned with any configuration whatsoever. The very design of real \texttt{bridges} (and thus of its virtual counterparts) takes care of this for us. We'll later see how this is not the case for network level routers. We'll need to manually populate the routing tables so that we attain the desired topology.

            We would also like to state that we did have to overcome a slight but hard-to-spot problem related to these bridges. It involves a setting controlling whether to involve \texttt{iptables} in frame forwarding at each of the virtual bridges. As this interaction has several important implications, such as violating a "pure" layered architecture we'll delve deeper into it at a later time. We just want to point out that things are not as easy as they seem most fo the time...

            Even though the previous paragraphs might make bridge creation to look cumbersome, it couldn't be any easier. Getting a working bridge is a matter of executing the lines found on listing \ref{lst:bridge-creation} from a shell. As listing \ref{lst:bridge-creation} contains commands that ought to be run in a shell we would like to discuss the necessary execution permissions so that someone trying to recreate this work can do so in a satisfactory way.

            \subsubsection{Capabilities for network infrastructure handling}
                Unleashing \texttt{iproute2}'s full potential implies that we can wreck havoc on a system's network infrastructure. We could leave the entire system without network connectivity or alter the way traffic is processed in such a way that we could snoop on user's data for example. This implies that not everybody using a system should be able to perform these actions.

                One of the ideas at the core of any system is \textit{access control}. It let's us control what system users can and can't do in such a way that we protect the system's stability and security. Each user will have different capabilities to perform actions on the system. One can read on them on \texttt{manpages} such as the one provided by running \texttt{man capabilities}, but the bottom line is that not every user will be able to use all of \texttt{iproute2}'s features without them being granted some capabilities. Given the configuration overhead this entails, we opted for issuing restricted commands as the system's \texttt{root} user whose \textit{user id} (\texttt{UID}) is \texttt{0}. This implies it bypasses all capability checks: that is, \texttt{root} as unrestricted access to \texttt{iproute2}'s features. On \texttt{UNIX}-based systems one can either opt for prepending \texttt{sudo} to commands such as the ones on listing \ref{lst:bridge-creation} or just run all of them as  \texttt{root} directly. The latter can be achieved by running \texttt{su -} to switch users. These are by no means the only ways of running the commands we include in this document. One can grant the required capabilities to an arbitrary user to perform the same tasks whilst being more elegant with respect to capability handling. Again, the information found on \texttt{man capabilities} is a wonderful stepping stone for more complex capability setups.

                We'll revisit capabilities when discussing the method of creating \textit{docker containers} later down the road as it's a crucial step for enabling the management of a container's networking infrastructure.

            \begin{lstlisting}[language = bash, caption = Instantiating a Virtual Network Bridge., label = lst:bridge-creation]
                # Create a new virtual bridge named foo-brd
                ip link add foo-brd type bridge

                # Bring it up (i.e. turn it on)
                ip link set foo-brd up

                # Check the bridge is indeed up
                ip link

                # This should provide an output like the following:
                # 4: foo-brd: <BROADCAST,MULTICAST,UP,LOWER_UP>\
                    # mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT
                    # group default qlen\
                    # link/ether 72:8a:77:68:42:b1 brd ff:ff:ff:ff:ff:ff
            \end{lstlisting}

            We would like to draw the reader's attention to the \texttt{LOWER\_UP} string found on \texttt{ip link}'s output on listing \ref{lst:bridge-creation}, as it's letting us know that the newly created \texttt{bridge} is indeed up. On top of that, we would also like to note that the backslash (\texttt{\textbackslash}) characters found on \texttt{ip link}'s output are \textbf{not} present on the "real" output. We have included them out of necessity, given the output lines were too long to be displayed in a single line in this document. We decided to indicate where we had broken up the line in an effort to be true to the original text whilst making it more visually appealing. This small "tweak" has been applied on other listings such as \ref{lst:veth-creation}.

        \subsection{\texttt{Veths}}
            Now that we know how to create \texttt{bridges} it's time to instantiate the "wires" connecting all of them together. Just like we explained before, these "wires" are supported on \texttt{veths}. The creation process is very similar to that of bridges in the sense that we'll leverage \texttt{ip link}'s functionality. Listing \ref{lst:veth-creation} shows how these \texttt{veths} are created. From \texttt{iproute2}'s perspective, these \texttt{veths} manifest as two different interfaces that are nonetheless very intimately related. Just like we explained before, everything that enters one \texttt{veth} end will go out the other and vice versa. This relation is made clear by how these \texttt{veths} are displayed when querying the network configuration as seen on listing \ref{lst:veth-creation}.

            \begin{lstlisting}[language = bash, caption = Instantiating a Virtual Ethernet Interface., label = lst:veth-creation]
                # Create a new veth whose ends are named veth-end-x and
                    # veth-end-y
                ip link add veth-end-x type veth peer name veth-end-y

                # Bring BOTH ENDS up
                ip link set veth-end-x up
                ip link set veth-end-y up

                # Check the veth ends are indeed up
                ip link

                # This should provide an output like the following:
                # 5: veth-end-y@veth-end-x: <BROADCAST,MULTICAST,UP,\
                    # LOWER_UP> mtu 1500 qdisc noqueue state UP mode\
                    # DEFAULT group default qlen 1000\
                    # link/ether 5e:b1:40:3f:e7:53 brd ff:ff:ff:ff:ff:ff
                # 6: veth-end-x@veth-end-y: <BROADCAST,MULTICAST,UP,\
                    # LOWER_UP> mtu 1500 qdisc noqueue state UP mode\
                    # DEFAULT group default qlen 1000\
                    # link/ether de:16:80:f6:c3:64 brd ff:ff:ff:ff:ff:ff
            \end{lstlisting}

            When looking at listing \ref{lst:veth-creation} we find that, just like in listing \ref{lst:bridge-creation}, the \texttt{LOWER\_UP} string is telling us that both \texttt{veth} ends are up and running. We would also like to bring the reader's attention to the \texttt{@veth-end-[x/y]} suffix in both \texttt{veth} names. This is just telling us the name of the other \texttt{veth} end associated with this one. In other words, \texttt{veth-end-y@veth-end-x} tells us that the \texttt{veth} whose configuration we are currently inspecting (i.e. \texttt{veth-end-y}) is attached to \texttt{veth-end-x}. Nonetheless, we need not specify this suffix when referring to the \texttt{veth} ends during configuration. Note we didn't include it on lines $5$ and $6$ on the same listing.

            \subsubsection{Adding \texttt{veths} to network namespaces}
                With what we have seen on the previous section we are only capable of creating the \texttt{veths} themselves. In order for them to be useful we need to somehow connect them to other network elements so that they are ``physically'' connected. The process of connecting \texttt{veth} depends on what element we are to connect it to. Even though it only takes a single command to do, the connection of \texttt{veths} is a more conceptually intricate process that requires us to recall what network namespaces were.

                When dealing with \texttt{bridges} we just need to issue a simple command as seen on listing \ref{lst:veth-brd-cnx}. Said instruction would be the ``virtual'' equivalent of walking up to a real \texttt{bridge} and plugging an \texttt{Ethernet} wire in, for instance.

                \begin{lstlisting}[language = bash, caption = Connecting a \texttt{veth} end to a virtual \texttt{bridge}., label = lst:veth-brd-cnx]
                    # Connect veth end veth-end-x to bridge foo-brd
                    ip link set veth-end-x master foo-brd
                \end{lstlisting}

                Now, connecting a \texttt{veth} to a host is a more intricate process. Even though it is true that it only requires a single command, we need be clear about the ``conceptual domain'' backing said connection up. Doing so requires recalling what \textit{network namespaces} were. In the realm of networking we stated that a network namespace could be regarded as an independent network stack within a single physical machine. Thus, each process that was granted its own would ``believe'' to have its own, independent network stack. We also teased that each of our network nodes (both hosts and routers) would run as a docker container and that each of them would have an associated namespace. Thus, connecting a \texttt{veth} to a host or router is a synonym for making said \texttt{veth} a part of that namespace. This can be accomplished through a single command as seen on listing \ref{lst:veth-namespace-cnx}. We would like to stress the fact that in order for said command to behave as expected we must ensure \texttt{iproute2} can ``see'' the namespace we are trying to add the \texttt{veth} to. This is a topic we will delve into in the following section.

                \begin{lstlisting}[language = bash, caption = Connecting a \texttt{veth} end to a host or router., label = lst:veth-namespace-cnx]
                    # Connect veth end veth-end-y to a host whose associated
                        # namespace is host-a-ns
                    ip link set veth-end-y netns host-a-ns
                \end{lstlisting}

        \subsection{Managing network namespaces}
            Given the framework we have designed we will need to circumvent some small limitations to leverage the \texttt{iproute2} suite. We will explained what these are and how to quench them before explaining how to work with \texttt{iproute2} in a ``multi-namespace'' setup.
            \subsubsection{Making \texttt{iproute2} aware of container namespaces}
                The command we presented on listing \ref{lst:veth-namespace-cnx} depends on \texttt{iproute2} being aware of the existence of the \texttt{host-a-ns} network namespace. This will not be an issue if the namespace itself is created with the same suite of tools. However, this is \textbf{not} our case: we are trying to interact with network namespaces our docker containers are managing themselves. This implies that we need to manually perform some actions to let \texttt{iproute2} manage these namespaces too.

                Working with virtual network infrastructure and namespaces is rather abstract. Thus, commands letting us inspect the current state of affairs are tremendously helpful. In this case we can leverage \texttt{ip netns}. This will list all the namespaces the \texttt{iproute2} suite is currently aware of (and thus, those it can currently manage and modify). If we \texttt{run} a docker container and compare the output of the \texttt{ip netns} command before and after doing so we \textbf{will not} appreciate any difference. This follows from the fact that, as we stated before, the associated namespace is managed by \texttt{docker} itself. After performing the actions we detail on the next paragraph one will be able to check how the output shown by \texttt{ip netns} does reflect the newly created namespace.

                The key idea to keep in mind is that ``a named network namespace is an object at \texttt{/var/run/netns/NAME} that can be opened'' as stated on \texttt{ip-netns}'s \textit{manpage}. Note that in order to be sure that we are obtaining information relevant for our platform we must make sure this \textit{manpage} is queried on a machine running \texttt{Ubuntu} or on a site offering \texttt{Ubuntu}'s \textit{manpages}. All in all, we find that in order for \texttt{iproute2} to be aware of network namespaces these must be available at \texttt{/var/run/netns}. If we create a network namespace ourselves with \texttt{ip netns add foo-ns} we would find that the \texttt{ip netns} command now shows \texttt{foo-ns} on its output and that there would be a new empty file at \texttt{/var/run/netns/foo-ns}. The latter is the ``manifestation'' of the newly created network namespace on our filesystem. As expected, \texttt{running} a \texttt{docker} container \textbf{does not} place any file under \texttt{/var/run/netns} and so \texttt{iproute2} cannot manage that namespace. Nonetheless, the fact that a container is associated with its own namespace \textbf{implies} a file similar to \texttt{foo-ns} must also exist somewhere in the filesystem once the container is active. We only need to find out were it is and link it to \texttt{/var/run/netns} so that we can work with the container's namespace.

                Even though we will devote a section to the management of containers, we will look a bit into the \texttt{docker inspect} command. This instruction lets us query information regarding a particular container. As of now, we are interested in the containers \texttt{PID} (\textbf{P}rocess \textbf{Id}entifier; remember a container \textit{virtualizes a process}: it's a single process with its own \texttt{PID}). We can easily retrieve said identifier based on the container's name through \texttt{docker inspect -f {{.State.Pid}} <container\_name>}. This \texttt{PID} will let us locate the container namespace under the \texttt{/proc} directory. The filesystem mounted at \texttt{/proc} is by no means a regular one. We will provide a very short description of its purpose and some example uses in the next section.

                \paragraph{The \texttt{procfs} interface}
                    Filesystems were initially envisioned to manage data in the form of files and directories. Nonetheless, the mechanisms provided by them can be leveraged to present other types of information. This is just what the \texttt{Proc Filesystem (procfs)} accomplishes. It presents information on the system's process and other characteristics in a hierarchical manner akin to how files are organized in a common filesystem such as \texttt{ext4}. This in turn provides a ways of communication between the \texttt{user} and \texttt{kernel} spaces that's leveraged by tools such as \texttt{ps} (\texttt{GNU}'s implementation doesn't use \texttt{system calls}, it just queries the \texttt{procfs} to obtain the necessary information). As containers are just processes, they also ``manifest'' under \texttt{/proc}. Through a container's \texttt{PID} we can locate the relevant information and obtain the empty file granting access to its namespace (this file is located at \texttt{/proc/<container\_pid>/ns/net} for a given container). This information has been extracted from \href{https://en.wikipedia.org/wiki/Procfs}{here}.

                In the light of all the above, we can summarize the process of making a container's namespace visible to \texttt{iproute2} in $3$ steps. We are also including a code snippet on listing \ref{lst:ns-link}. If we run \texttt{ip netns} after carrying these out we must be able to find the container's namespace in the output. We are now ready to modify it as we please.

                \begin{enumerate}
                    \item Obtain the container's \texttt{PID}.
                    \item Find the file representing its network namespace under \texttt{/proc}.
                    \item Create a link to said file under \texttt{/var/run/netns}.
                \end{enumerate}

                \begin{lstlisting}[language = bash, caption = Linking a container's network namespace to \texttt{/var/run/netns}., label = lst:ns-link]
                    # Find out the PID of container foo-cont
                    cont_pid=$(docker inspect -f {{.State.Pid}} foo-cont)

                    # Create the link under /var/run/netns. The namespace will
                        # show up as foo-cont when running ip netns. The name
                        # we'll find is that of the link we create. It is NOT
                        # compulsory to use the container's name, but it
                        # does make namespace management easier.
                    # Options used with ln:
                        # -s: Create a symbolic link
                        # -f: Force link creation (i.e. overwrite an existing link)
                    ln -sf /proc/$(cont_pid)/ns/net /var/run/netns/foo-cont

                    # This can also be accomplished in a single line
                    # ln -sf /proc/$(docker inspect -f {{.State.Pid}} foo-cont)/ns/net\ 
                        # /var/run/netns/foo-cont
                \end{lstlisting}

            \subsubsection{Running commands on different namespaces}
                If we recall listing \ref{lst:veth-namespace-cnx}, we notice how connecting a \texttt{veth} end to a host is equivalent to ``moving'' that interface to the host's \texttt{namespace}. However, we did not delve much deeper into the implications of this action. Once an interface is sent to a \texttt{namespace} different than the default one (i.e. the \texttt{root namespace}) we'll find that running commands like \texttt{ip link} or \texttt{ip addr} won't show that interface anymore.

                We need to be aware that, even though we don't usually specify it, every \texttt{ip XYZ} command is running within a given \texttt{namespace}. Up until now, these were being executed on the \texttt{default} namespace as we weren't specifying the opposite. We then need to, after sending an interface to a different namespace, somehow instruct subsequent commands to run within that new namespace instead of the default one. This can be easily accomplished thanks to the \texttt{-n} flag accepted by commands from the \texttt{iproute2} suite. Listing \ref{lst:netns-cmds} contains a series of commands that portray this behaviour. Please note that for the sake of brevity we assume the \texttt{veths} created on listing \ref{lst:veth-creation} still exist.

                \begin{lstlisting}[language = bash, caption = Running \texttt{ip} on a different namespace., label = lst:netns-cmds]
                    # Create the host-a-ns namespace
                    ip netns add host-a-ns

                    # Move veth-end-y to the host-a-netns namespace
                    ip link set veth-end-y netns host-a-ns

                    # Analyze the current interfaces on the default
                        # namespace
                    ip link

                    # This should provide an output like the following:
                    # 6: veth-end-x@if5: <BROADCAST,MULTICAST,UP,\
                        # LOWER_UP> mtu 1500 qdisc noqueue state UP mode\
                        # DEFAULT group default qlen 1000\
                        # link/ether de:16:80:f6:c3:64 brd ff:ff:ff:ff:ff:ff

                    # Do the proper thing on the host-a-ns namespace
                    ip -n host-a-ns link

                    # This should produce an output in line with the following:
                    # 5: veth-end-y@if6: <BROADCAST,MULTICAST,UP,\
                        # LOWER_UP> mtu 1500 qdisc noqueue state UP mode\
                        # DEFAULT group default qlen 1000\
                        # link/ether 5e:b1:40:3f:e7:53 brd ff:ff:ff:ff:ff:ff
                \end{lstlisting}

                Notice how after moving a \texttt{veth} ned to a different namespace on listing \ref{lst:netns-cmds}, we can no longer ``see it'' when running \texttt{ip link} on the \texttt{default namespace}. We would also like to point out that we had to manually create the \texttt{host-a-ns namespace} for demonstrative purposes but this \textbf{will not} be the case when we are dealing with the framework we have developed. Namespaces will be managed by \texttt{docker} in their entirety so we don't have to be concerned about their creation and latter deletion.

    \section{Addressing Layer 3 Network Devices}
        In previous sections we have covered how to instantiate different virtual network elements. Even though we haven't exactly looked into \textbf{how} to leverage \texttt{docker} to generate virtual hosts in the form of \texttt{containers} we can safely assume that is something we can indeed achieve. What's more, we have discovered how, from the networking point of view it suffices to create a network namespace: we don't need anything else more than that to generate a presence on the network. It is true that a namespace is a lifeless being in the sense that it won't generate any traffic or reply to any request: it only provides the network backbone that the containers will indeed use. Nonetheless, we already have all the information we need to tackle the topic of addressing.

        This section is concerned with \texttt{layer 3} of the \texttt{OSI Model} (i.e. the \texttt{network layer}). The network stacks we are dealing with implement \texttt{IP} at this level, so when we refer to addressing a host it's equivalent to assigning it an \texttt{IP address}. Now, we need to be aware of the fact that we are \textbf{not} addressing \texttt{hosts}: we are addressing \texttt{interfaces} belonging to that host. From an end user's perspective it's quite common for a given network-aware machine to contain a single network interface. The fact that there sometimes exists a one-to-one correspondence between machine and interface \textbf{does not imply} that addressing a host is the same as addressing an interface. The clearest example of this would be the way we deal with \texttt{layer 3 packet switches} (i.e. \texttt{routers}). These will (usually) contain more than one interface where each of them belongs to a different \texttt{subnetwork}. Then, each of them needs a different network address. Now, what's the router's \texttt{IP} address? We know it has at least two different addresses, so what's the correct answer? This example shows how the misconceptions surrounding \texttt{IP} addressing can be easily dismantled.

        In the previous paragraph we found out how we are actually addressing \texttt{interfaces}, not \texttt{systems} themselves. Once we have cleared the ``conceptual'' air we will find out how it's rather simple to address interfaces thanks to the \texttt{ip addr}. Listing \ref{lst:addr-iface} shows the process of assigning a given \texttt{IP} address to a given interface. This listing assumes the interfaces created on listing \ref{lst:veth-creation} still exist.

        \begin{lstlisting}[language = bash, caption = Addressing an inetrface., label = lst:addr-iface]
            # Assign the 192.168.1.1/24 address to the veth-end-x veth end. This command will automatically
                # assign a broadcast address based on the provided subnet mask (i.e. /24)
            ip addr add 192.168.1.1/24 brd + dev veth-end-x

            # Verify the changes were applied
            ip a

            # We should see something like the following:
            # 5: veth-end-x@if4: <NO-CARRIER,BROADCAST,MULTICAST,\
                # UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN\
                # group default qlen 1000
                # link/ether de:16:80:f6:c3:64 brd ff:ff:ff:ff:ff:ff\
                # link-netns host-a-ns
                # inet 192.168.1.1/24 brd 192.168.1.255 scope global\
                # veth-end-x
                    # valid_lft forever preferred_lft forever
                # inet6 fe80::cd4:77ff:fe8a:2ec0/64 scope link
                    # valid_lft forever preferred_lft forever

            # We can of course address interfaces on other namespaces
                # with the -n flag. The following would assign address
                # 192.168.1.2 to the veth-end-y present on the
                # host-a-ns namespace.
            ip -n host-a-ns addr add 192.168.1.2/24 brd + dev veth-end-y
        \end{lstlisting}

        We would like to mention that assigning addresses like we do on listing \ref{lst:addr-iface} will also affect the route table of the machine the interface belongs to. We'll delve deeper into this topic when we discuss routing in a later section.

    